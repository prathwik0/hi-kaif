import httpx
from typing import Dict, Any
import urllib.parse


async def get_page_content(page_title: str) -> Dict[str, Any]:
    """Get the content of a Wikipedia page with images."""
    try:
        api_url = "https://en.wikipedia.org/w/api.php"
        headers = {
            "User-Agent": "WikipediaSearchTool/1.0 (https://example.com/contact)"
        }

        # Get the page content
        extract_params = {
            "action": "query",
            "format": "json",
            "titles": page_title,
            "prop": "extracts",
            "explaintext": True,
            "exsectionformat": "plain",
            "exlimit": "max",
        }

        # Get page images
        image_params = {
            "action": "query",
            "format": "json",
            "titles": page_title,
            "generator": "images",
            "gimlimit": 15,  # Limit images to 15
            "prop": "imageinfo",
            "iiprop": "url|extmetadata",
            "iiurlwidth": 1024,
            "iiextmetadatalang": "en",
        }

        async with httpx.AsyncClient(timeout=10.0) as client:
            # Get page content
            response = await client.get(api_url, params=extract_params, headers=headers)
            response.raise_for_status()
            extract_data = response.json()

            image_response = await client.get(
                api_url, params=image_params, headers=headers
            )
            image_response.raise_for_status()
            image_data = image_response.json()

        # Process content
        extract_text = ""
        pages = extract_data.get("query", {}).get("pages", {})
        for page_id, page_data in pages.items():
            if page_id != "-1" and "extract" in page_data:
                extract_text = page_data.get("extract", "No content available")
                break

        # Process images
        images: list[Dict[str, Any]] = []
        seen_urls = set()
        image_pages = image_data.get("query", {}).get("pages", {})

        def add_image(url: str, description: str):
            if url and url not in seen_urls:
                images.append({"url": url, "description": description})
                seen_urls.add(url)

        for page_id, page_data in image_pages.items():
            if page_id == "-1":
                continue

            # This is an image page generated by the generator
            info = (page_data.get("imageinfo") or [{}])[0]
            if info:
                # Extract description from metadata
                extmetadata = info.get("extmetadata", {})
                description = (
                    extmetadata.get("ImageDescription", {}).get("value")
                    or extmetadata.get("ObjectName", {}).get("value")
                    or extmetadata.get("Caption", {}).get("value")
                    or page_data.get("title", "").replace("File:", "").replace("_", " ")
                )

                # Add original URLs (exclude SVG images)
                orig_url = info.get("url")
                if orig_url and not orig_url.lower().endswith(".svg"):
                    add_image(orig_url, description or "Image")

        if not extract_text:
            return {
                "content": "Page not found or no content available",
                "images": images,
            }

        return {"content": extract_text, "images": images}

    except httpx.RequestError as e:
        error_msg = f"Network error fetching page content for '{page_title}': {str(e)}"
        print(error_msg, flush=True)
        return {"content": error_msg, "images": []}
    except Exception as e:
        error_msg = f"Error fetching page content for '{page_title}': {str(e)}"
        print(error_msg, flush=True)
        return {"content": error_msg, "images": []}


async def search_wikipedia(query: str, limit: int = 5) -> Dict[str, Any]:
    """Search Wikipedia for the given query and return results"""
    try:
        search_url = "https://en.wikipedia.org/w/api.php"
        search_params = {
            "action": "query",
            "list": "search",
            "srsearch": query,
            "format": "json",
            "srlimit": min(limit, 50),  # Wikipedia API limits to 50 max
        }

        headers = {
            "User-Agent": "WikipediaSearchTool/1.0 (https://example.com/contact)"
        }

        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(
                search_url, params=search_params, headers=headers
            )
            response.raise_for_status()
            search_data = response.json()

        if "query" not in search_data or "search" not in search_data["query"]:
            return {
                "wikipedia_search": True,
                "search_query": query,
                "results": [],
                "total_results": 0,
                "error": "No search results found",
            }

        search_results = search_data["query"]["search"]
        results = []

        for result in search_results[:limit]:
            # Get page content and images for each search result
            page_title = result["title"]
            page_data = await get_page_content(page_title)

            encoded_title = urllib.parse.quote(page_title.replace(" ", "_"))
            page_url = f"https://en.wikipedia.org/wiki/{encoded_title}"

            results.append(
                {
                    "title": page_title,
                    "snippet": result.get("snippet", ""),
                    "pageid": result.get("pageid"),
                    "wordcount": result.get("wordcount", 0),
                    "timestamp": result.get("timestamp", ""),
                    "url": page_url,
                    "content": page_data.get("content", ""),
                    "images": page_data.get("images", []),
                }
            )

        if results:
            print(
                f"Wikipedia search successful: {query} -> {len(results)} results",
                flush=True,
            )

        return {
            "wikipedia_search": True,
            "search_query": query,
            "results": results,
            "total_results": len(results),
            "success": True,
        }

    except httpx.RequestError as e:
        error_msg = f"Network error searching Wikipedia: {str(e)}"
        print(error_msg, flush=True)
        return {
            "wikipedia_search": True,
            "search_query": query,
            "results": [],
            "total_results": 0,
            "error": error_msg,
            "success": False,
        }
    except Exception as e:
        error_msg = f"Error searching Wikipedia: {str(e)}"
        print(error_msg, flush=True)
        return {
            "wikipedia_search": True,
            "search_query": query,
            "results": [],
            "total_results": 0,
            "error": error_msg,
            "success": False,
        }


def get_wikipedia_tool_definition() -> Dict[str, Any]:
    """Get the tool definition for function calling."""
    return {
        "name": "wikipedia_search",
        "description": "Search Wikipedia for information about a topic or query.",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "The search query or topic to search for on Wikipedia",
                },
                "limit": {
                    "type": "integer",
                    "description": "Maximum number of search results to return (default: 5)",
                    "default": 5,
                },
            },
            "required": ["query"],
        },
    }
